{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据及环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自带数据且已经解压，忽略前两个代码块\n",
    "from modelarts.session import Session\n",
    "session = Session()\n",
    "\n",
    "if session.region_name == 'cn-north-4':\n",
    "    bucket_path = 'professional-construction/NLP/kbqa.zip'\n",
    "else:\n",
    "    print(\"请更换地区到北京四\")\n",
    "    \n",
    "session.download_data(bucket_path=bucket_path, path='./kbqa.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  kbqa.zip\n",
      "   creating: kbqa/\n",
      "   creating: kbqa/bert/\n",
      "   creating: kbqa/bert/bert_zh_L-12_H-768_A-12_2/\n",
      "   creating: kbqa/bert/bert_zh_L-12_H-768_A-12_2/assets/\n",
      "  inflating: kbqa/bert/bert_zh_L-12_H-768_A-12_2/assets/vocab.txt  \n",
      "  inflating: kbqa/bert/bert_zh_L-12_H-768_A-12_2/saved_model.pb  \n",
      "   creating: kbqa/bert/bert_zh_L-12_H-768_A-12_2/variables/\n",
      "  inflating: kbqa/bert/bert_zh_L-12_H-768_A-12_2/variables/variables.data-00000-of-00001  \n",
      "  inflating: kbqa/bert/bert_zh_L-12_H-768_A-12_2/variables/variables.index  \n",
      "   creating: kbqa/Data/\n",
      "  inflating: kbqa/Data/construct_dataset_attribute.py  \n",
      "  inflating: kbqa/Data/construct_dataset_ner.py  \n",
      "   creating: kbqa/Data/NER_Data/\n",
      "  inflating: kbqa/Data/NER_Data/test.txt  \n",
      "  inflating: kbqa/Data/NER_Data/train.txt  \n",
      "   creating: kbqa/Data/NLPCC2016KBQA/\n",
      "  inflating: kbqa/Data/NLPCC2016KBQA/nlpcc-iccpol-2016.kbqa.kb  \n",
      "  inflating: kbqa/Data/NLPCC2016KBQA/nlpcc-iccpol-2016.kbqa.testing-data  \n",
      "  inflating: kbqa/Data/NLPCC2016KBQA/nlpcc-iccpol-2016.kbqa.training-data  \n",
      "   creating: kbqa/Data/Sim_Data/\n",
      "  inflating: kbqa/Data/Sim_Data/dev.txt  \n",
      "  inflating: kbqa/Data/Sim_Data/test.txt  \n",
      "  inflating: kbqa/Data/Sim_Data/train.txt  \n",
      "  inflating: kbqa/Data/test.csv      \n",
      "  inflating: kbqa/Data/train.csv     \n",
      "   creating: kbqa/output_ner/\n",
      "   creating: kbqa/output_sim/\n"
     ]
    }
   ],
   "source": [
    "! unzip kbqa.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-hub\n",
      "  Using cached tensorflow_hub-0.13.0-py2.py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./miniconda3/envs/tf/lib/python3.8/site-packages (from tensorflow-hub) (1.24.2)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in ./miniconda3/envs/tf/lib/python3.8/site-packages (from tensorflow-hub) (3.19.6)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.13.0\n",
      "Collecting bert-for-tf2\n",
      "  Using cached bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py-params>=0.9.6\n",
      "  Using cached py-params-0.10.2.tar.gz (7.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting params-flow>=0.8.0\n",
      "  Using cached params-flow-0.8.2.tar.gz (22 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./miniconda3/envs/tf/lib/python3.8/site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.24.2)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
      "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30516 sha256=b9c8695146fc8181ba81d3814d1e81714a8f7c71d930ec1f1c52d697c57cff22\n",
      "  Stored in directory: /home/fangguian_i2023/.cache/pip/wheels/ab/a4/72/df07592cea3ae06b5e846f5e52262f8b16748e829ca354b7df\n",
      "  Building wheel for params-flow (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19457 sha256=9ac420967bf582908b3d36c3667b682e06d847d35757fdf9f2a557ba8845eb1c\n",
      "  Stored in directory: /home/fangguian_i2023/.cache/pip/wheels/c7/f3/85/b8cf1d8bfe55dc2ece0f1fcd4e91d6f8fc7b59ff3fd75329e1\n",
      "  Building wheel for py-params (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7890 sha256=d6b0d8526d44b303ae09f705bcc482dff245e8b0205019421fc6f72518b1e27a\n",
      "  Stored in directory: /home/fangguian_i2023/.cache/pip/wheels/ac/26/e9/df16869ccbd4abf517f1ff3be9a2c7ee5c5980fc87eea04fb1\n",
      "Successfully built bert-for-tf2 params-flow py-params\n",
      "Installing collected packages: tqdm, py-params, params-flow, bert-for-tf2\n",
      "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2 tqdm-4.65.0\n",
      "Collecting seqeval\n",
      "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in ./miniconda3/envs/tf/lib/python3.8/site-packages (from seqeval) (1.24.2)\n",
      "Collecting scikit-learn>=0.21.3\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.3.2\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16164 sha256=0eda0ee65fb6fb024433eff45544d3134f045d592ab89ac528b5da71b8eb253c\n",
      "  Stored in directory: /home/fangguian_i2023/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
      "Successfully built seqeval\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, seqeval\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.2 scipy-1.10.1 seqeval-1.2.2 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow-hub\n",
    "! pip install bert-for-tf2\n",
    "! pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in ./miniconda3/envs/tf/lib/python3.9/site-packages (2.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (16.0.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: setuptools in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (66.0.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (1.54.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (23.3.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (2.11.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: six>=1.12.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.20 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.17.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./miniconda3/envs/tf/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "# 装好上述内容之后没有TensorFlow 2.1.0，需要手动安装 查询不到2.1.0，只能安装2.11.0，后续可能因为版本出现问题\n",
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 知识库问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # TensorFlow的warning信息太多，这里屏蔽掉\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from bert import bert_tokenization\n",
    "from tensorflow.keras.models import load_model\n",
    "from seqeval.metrics.sequence_labeling import get_entities, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_format_file(file_path: str,\n",
    "                           text_index: int = 0,\n",
    "                           label_index: int = 1):\n",
    "    \"\"\"\n",
    "    conll 格式数据读取\n",
    "    Args:\n",
    "        file_path: 文件路径\n",
    "        text_index: 输入所在列的索引，默认是0，第一列\n",
    "        label_index: 标签所在列的索引，默认是1，第二列\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    x_data, y_data = [], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        x, y = [], []\n",
    "        for line in lines:\n",
    "            rows = line.split()\n",
    "            if len(rows) == 0: # 如果当前行为空格，则表示上一句结束，把上一句的序列数据加入到数据中\n",
    "                x_data.append(x)\n",
    "                y_data.append(y)\n",
    "                x = []\n",
    "                y = []\n",
    "            else: # 添加输入和标签\n",
    "                x.append(rows[text_index])\n",
    "                y.append(rows[label_index])\n",
    "                \n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['《', '高', '等', '数', '学', '》', '是', '哪', '个', '出', '版', '社', '出', '版', '的', '？']\n",
      "['O', 'B-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = read_conll_format_file('./kbqa/Data/NER_Data/train.txt')\n",
    "test_x, test_y = read_conll_format_file('./kbqa/Data/NER_Data/test.txt')\n",
    "\n",
    "print(train_x[1])\n",
    "print(train_y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_NER_Config():\n",
    "    max_seq_length = 64  # 输入序列的最大长度，用于把输入padding成统一长度\n",
    "    bert_dir = './kbqa/bert/bert_zh_L-12_H-768_A-12_2' # bert预训练模型的路径\n",
    "    epochs = 8 # 训练轮数\n",
    "    batch_size = 256 \n",
    "    \n",
    "bert_ner_config = BERT_NER_Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理（文字->id）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_Preprocessor(object):\n",
    "    \"\"\"\n",
    "    预处理类，字->id, 标签->id\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        初始化，加载分词器，此处是按字符切词\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.label_dict = {'O':0, 'B-ENT':1, 'I-ENT':2}\n",
    "        self.idx2label = {0: 'O', 1: 'B-ENT', 2: 'I-ENT'}\n",
    "        self._tokenizer = bert_tokenization.FullTokenizer(vocab_file=config.bert_dir+'/assets/vocab.txt', do_lower_case=False)\n",
    "        \n",
    "    def get_masks(self, tokens):\n",
    "        \"\"\"获取 BERT mask id 输入\"\"\"\n",
    "\n",
    "        return [1]*len(tokens)\n",
    "\n",
    "    def get_segments(self, tokens):\n",
    "        \"\"\"获取 BERT segments id 输入，第一句用0表示，第二句用1表示\"\"\"\n",
    "\n",
    "        segments = []\n",
    "        current_segment_id = 0\n",
    "        for token in tokens:\n",
    "            segments.append(current_segment_id)\n",
    "            if token == \"[SEP]\":\n",
    "                current_segment_id = 1\n",
    "        return segments\n",
    "\n",
    "    def get_ids(self, tokens):\n",
    "        \"\"\"获取 BERT 字符id 输入\"\"\"\n",
    "        token_ids = [self._tokenizer.vocab.get(token, self._tokenizer.vocab['[UNK]']) for token in tokens]\n",
    "\n",
    "        return token_ids\n",
    "    \n",
    "    def transform(self, stokens):\n",
    "        \"\"\"将词序列转换成BERT需要的字符id、mask id、segments id\"\"\"\n",
    "        stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "        input_ids = self.get_ids(stokens)\n",
    "        input_masks = self.get_masks(stokens)\n",
    "        input_segments = self.get_segments(stokens)\n",
    "        \n",
    "        return input_ids, input_masks, input_segments\n",
    "    \n",
    "    def conll_transform(self, x_train, y_train=None):\n",
    "        \"\"\"训练数据批量转换\"\"\"\n",
    "        all_input_ids, all_input_masks, all_input_segments = [], [], []\n",
    "        for stokens in x_train:\n",
    "            input_ids, input_masks, input_segments = self.transform(stokens)\n",
    "            all_input_ids.append(input_ids)\n",
    "            all_input_masks.append(input_masks)\n",
    "            all_input_segments.append(input_segments)\n",
    "            \n",
    "        ## 输入数据统一padding成统一长度，以矩阵的形式送入模型    \n",
    "        all_input_ids = tf.keras.preprocessing.sequence.pad_sequences(all_input_ids, maxlen=self.config.max_seq_length, padding='post')\n",
    "        all_input_masks = tf.keras.preprocessing.sequence.pad_sequences(all_input_masks, maxlen=self.config.max_seq_length, padding='post')\n",
    "        all_input_segments = tf.keras.preprocessing.sequence.pad_sequences(all_input_segments, maxlen=self.config.max_seq_length, padding='post')\n",
    "            \n",
    "        if y_train:\n",
    "            all_label_ids = []\n",
    "            for slabels in y_train:\n",
    "                slabels = ['O'] + slabels + ['O']\n",
    "                slabel_ids = [self.label_dict[label] for label in slabels]\n",
    "                all_label_ids.append(slabel_ids)\n",
    "                \n",
    "            all_label_ids = tf.keras.preprocessing.sequence.pad_sequences(all_label_ids, maxlen=self.config.max_seq_length, padding='post')\n",
    "                \n",
    "            return [all_input_ids, all_input_masks, all_input_segments], all_label_ids\n",
    "        else:\n",
    "            return [all_input_ids, all_input_masks, all_input_segments]\n",
    "              \n",
    "    def online_transform(self, text):\n",
    "        \"\"\"单个文本在线转换\"\"\"\n",
    "        stokens = self._tokenizer.tokenize(text)\n",
    "        input_ids, input_masks, input_segments = self.transform(stokens)\n",
    "        \n",
    "        ## 输入数据padding成统一长度\n",
    "        input_ids = tf.keras.preprocessing.sequence.pad_sequences([input_ids], maxlen=self.config.max_seq_length, padding='post')\n",
    "        input_masks = tf.keras.preprocessing.sequence.pad_sequences([input_masks], maxlen=self.config.max_seq_length, padding='post')\n",
    "        input_segments = tf.keras.preprocessing.sequence.pad_sequences([input_segments], maxlen=self.config.max_seq_length, padding='post')\n",
    "        \n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "    def label_inverse_transform(self, label_ids, lengths=None):\n",
    "        \"\"\"标签id转换回标签名\"\"\"\n",
    "        result = []\n",
    "        for index, seq in enumerate(label_ids):\n",
    "            labels_ = []\n",
    "            for idx in seq:\n",
    "                labels_.append(self.idx2label[idx])\n",
    "            if lengths is not None:\n",
    "                labels_ = labels_[1:lengths[index] + 1]\n",
    "            else:\n",
    "                labels_ = labels_[1:-1]\n",
    "            result.append(labels_)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: [[ 101  517 7770 5023 3144 2110  518 8043  102    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n",
      "masks: [[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "segments: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "ner_preprocessor = NER_Preprocessor(bert_ner_config)\n",
    "ids, masks, segments = ner_preprocessor.online_transform(\"《高等数学》？\")\n",
    "print(\"id: {}\".format(ids))\n",
    "print(\"masks: {}\".format(masks))\n",
    "print(\"segments: {}\".format(segments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_LSTM_NER():\n",
    "    def __init__(self, config):\n",
    "        \"\"\"初始化预处理器\"\"\"\n",
    "        self._config = config # 配置参数\n",
    "        self._preprocessor = NER_Preprocessor(config) # 预处理器\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"搭建模型架构\"\"\"\n",
    "        # 定义bert模型输入，包括字符id, mask id, segment id\n",
    "        max_seq_length = self._config.max_seq_length\n",
    "        input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                               name=\"input_word_ids\")\n",
    "        input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                           name=\"input_mask\")\n",
    "        segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                            name=\"segment_ids\")\n",
    "        bert_layer = hub.KerasLayer(self._config.bert_dir, trainable=False) # tensorflow hub加载bert并转化为keras层\n",
    "        pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "        # 取bert的序列输出sequence_output, 后接bilstm\n",
    "        bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128,return_sequences=True))(sequence_output)\n",
    "        \n",
    "        dense = tf.keras.layers.Dense(units=64, activation='tanh', name='dense_layer_1')(bilstm)\n",
    "        \n",
    "        num_class = len(self._preprocessor.label_dict)\n",
    "        output = tf.keras.layers.Dense(units=num_class, activation='softmax', name='output')(dense)\n",
    "        \n",
    "\n",
    "        model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=output)\n",
    "        \n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def fit(self, x_train, y_train, x_valid=None, y_valid=None):\n",
    "        \"\"\"训练\"\"\"\n",
    "        \n",
    "        self.build_model()\n",
    "        self.model.summary()\n",
    "        x_train_processed, y_train_processed = self._preprocessor.conll_transform(x_train, y_train)\n",
    "        \n",
    "        if x_valid is not None and y_valid is not None: # 验证集存在\n",
    "            x_valid_preprocessed, y_valid_preprocessed = self._preprocessor.conll_transform(x_valid, y_valid)\n",
    "            self.model.fit(x_train_processed, y_train_processed, \n",
    "                           validation_data=(x_valid_preprocessed, y_valid_preprocessed),\n",
    "                           epochs=self._config.epochs,\n",
    "                           batch_size=self._config.batch_size,\n",
    "                           shuffle=True)\n",
    "        else: # 验证集不存在\n",
    "            self.model.fit(x_train_processed, y_train_processed, \n",
    "                           epochs=self._config.epochs,\n",
    "                           batch_size=self._config.batch_size,\n",
    "                           shuffle=True)\n",
    "            \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        \"\"\"测试评估\"\"\"\n",
    "        x_test_preprocessed = self._preprocessor.conll_transform(x_test)\n",
    "        y_preds = self.model.predict(x_test_preprocessed)\n",
    "        y_preds = np.argmax(y_preds, -1)\n",
    "        lengths = [len(seq) for seq in y_test]\n",
    "        y_preds = self._preprocessor.label_inverse_transform(y_preds, lengths) #预测的id转换回标签名\n",
    "        \n",
    "        new_y_trues, new_y_preds = [], []\n",
    "        for y_true_seq, y_pred_seq in zip(y_test, y_preds):\n",
    "            new_y_true_seq, new_y_pred_seq = [], []\n",
    "            for y_true, y_pred in zip(y_true_seq, y_pred_seq):\n",
    "                new_y_true_seq.append(str(y_true))\n",
    "                new_y_pred_seq.append(str(y_pred))\n",
    "            new_y_trues.append(new_y_true_seq)\n",
    "            new_y_preds.append(new_y_pred_seq)\n",
    "            \n",
    "        print(classification_report(new_y_trues, new_y_preds)) # 输出precision, recall, f1\n",
    "        \n",
    "    def inference(self, text):\n",
    "        \"\"\"推理\"\"\"\n",
    "        inputs = self._preprocessor.online_transform(text)\n",
    "        y_preds = self.model.predict(inputs)\n",
    "        y_preds = np.argmax(y_preds, -1)\n",
    "        y_preds = self._preprocessor.label_inverse_transform(y_preds)\n",
    "        print(y_preds)\n",
    "        entities = get_entities(y_preds[0])\n",
    "        format_entites = []\n",
    "        for entity in entities:\n",
    "            value = text[entity[1]:entity[2] + 1]\n",
    "\n",
    "            # 输入实体类型， 开始位置，结束位置，实体值\n",
    "            format_entites.append({\n",
    "                \"entity\": entity[0],\n",
    "                \"start\": entity[1],\n",
    "                \"end\": entity[2],\n",
    "                \"value\": value,\n",
    "            })\n",
    "\n",
    "        return {\"text\": text, \"entities\": format_entites}\n",
    "            \n",
    "    def save(self, model_save_dir):\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        model_path = os.path.join(model_save_dir, \"model.h5\")\n",
    "        self.model.save_weights(model_path)\n",
    "    \n",
    "    def restore(self, model_dir):\n",
    "        \"\"\" 加载模型\"\"\"\n",
    "        self.build_model()\n",
    "        self.model.load_weights(os.path.join(model_dir, \"model.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#改进模型\n",
    "#!pip install keras_nlp\n",
    "from transformers import TFRobertaModel\n",
    "from tensorflow.keras.layers import Input, Dense, TimeDistributed, LSTM, Bidirectional, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_nlp import layers\n",
    "from keras_nlp import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "class Roberta_BiLSTM_CRF():\n",
    "    def __init__(self, config):\n",
    "        \"\"\"初始化预处理器\"\"\"\n",
    "        self._config = config # 配置参数\n",
    "        self._preprocessor = NER_Preprocessor(config) # 预处理器\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"搭建模型架构\"\"\"\n",
    "        # 定义roberta模型输入，包括字符id, mask id, segment id\n",
    "        max_seq_length = self._config.max_seq_length\n",
    "        input_word_ids = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "        input_mask = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n",
    "        segment_ids = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n",
    "        roberta = TFRobertaModel.from_pretrained('roberta-base')\n",
    "        sequence_output = roberta([input_word_ids, input_mask, segment_ids]).last_hidden_state\n",
    "        # 取roberta的序列输出sequence_output, 后接bilstm\n",
    "        bilstm = Bidirectional(LSTM(units=128, return_sequences=True))(sequence_output)\n",
    "        output = TimeDistributed(Dense(len(self._preprocessor.label_dict), activation='softmax'))(bilstm)\n",
    "        \n",
    "        model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=output)\n",
    "        \n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def fit(self, x_train, y_train, x_valid=None, y_valid=None):\n",
    "        \"\"\"训练\"\"\"\n",
    "        \n",
    "        self.build_model()\n",
    "        self.model.summary()\n",
    "        x_train_processed, y_train_processed = self._preprocessor.conll_transform(x_train, y_train)\n",
    "        y_train_processed = tf.one_hot(y_train_processed, depth=len(self._preprocessor.label_dict))\n",
    "        self.model.fit(x_train_processed, y_train_processed, epochs=8, batch_size=32)\n",
    "\n",
    "            \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        \"\"\"测试评估\"\"\"\n",
    "        x_test_preprocessed = self._preprocessor.conll_transform(x_test)\n",
    "        y_preds = self.model.predict(x_test_preprocessed)\n",
    "        y_preds = np.argmax(y_preds, -1)\n",
    "        lengths = [len(seq) for seq in y_test]\n",
    "        y_preds = self._preprocessor.label_inverse_transform(y_preds, lengths)\n",
    "        y_test = self._preprocessor.label_inverse_transform(y_test, lengths)\n",
    "        print(classification_report(y_test, y_preds)) # 输出precision, recall, f1\n",
    "\n",
    "    def save(self, model_dir):\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        self.model.save_weights(model_dir + 'model.h5')\n",
    "        with open(model_dir + 'preprocessor.pkl', 'wb') as f:\n",
    "            pickle.dump(self._preprocessor, f)\n",
    "\n",
    "    def restore(self, model_dir):\n",
    "        \"\"\"加载模型\"\"\"\n",
    "        with open(model_dir + 'preprocessor.pkl', 'rb') as f:\n",
    "            self._preprocessor = pickle.load(f)\n",
    "        self.build_model()\n",
    "        self.model.load_weights(model_dir + 'model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练，评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_word_ids (InputLayer)    [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_word_ids[0][0]',         \n",
      " el)                            thPoolingAndCrossAt               'input_mask[0][0]',             \n",
      "                                tentions(last_hidde               'segment_ids[0][0]']            \n",
      "                                n_state=(None, 64,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 64, 256)      918528      ['tf_roberta_model[0][0]']       \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 64, 3)       771         ['bidirectional[0][0]']          \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 125,564,931\n",
      "Trainable params: 125,564,931\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/8\n",
      "427/427 [==============================] - 2191s 5s/step - loss: 0.2166 - categorical_accuracy: 0.9120\n",
      "Epoch 2/8\n",
      "427/427 [==============================] - 2170s 5s/step - loss: 0.1734 - categorical_accuracy: 0.9221\n",
      "Epoch 3/8\n",
      "427/427 [==============================] - 2167s 5s/step - loss: 0.1681 - categorical_accuracy: 0.9231\n",
      "Epoch 4/8\n",
      "427/427 [==============================] - 2167s 5s/step - loss: 0.1646 - categorical_accuracy: 0.9243\n",
      "Epoch 5/8\n",
      "427/427 [==============================] - 2168s 5s/step - loss: 0.1640 - categorical_accuracy: 0.9240\n",
      "Epoch 6/8\n",
      "427/427 [==============================] - 2151s 5s/step - loss: 0.1616 - categorical_accuracy: 0.9254\n",
      "Epoch 7/8\n",
      "427/427 [==============================] - 2106s 5s/step - loss: 0.1609 - categorical_accuracy: 0.9254\n",
      "Epoch 8/8\n",
      "427/427 [==============================] - 2110s 5s/step - loss: 0.1617 - categorical_accuracy: 0.9251\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # 注：放的位置也会影响效果，真是奇妙的代码\n",
    "#bert_ner = BERT_LSTM_NER(bert_ner_config) #改进模型\n",
    "bert_ner = Roberta_BiLSTM_CRF(bert_ner_config)\n",
    "bert_ner.fit(train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 262s 922ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ENT       0.96      0.97      0.97      9030\n",
      "\n",
      "   micro avg       0.96      0.97      0.97      9030\n",
      "   macro avg       0.96      0.97      0.97      9030\n",
      "weighted avg       0.96      0.97      0.97      9030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_ner.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ner.save('kbqa/output_ner/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型加载，推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "[['O', 'B-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': '《高等数学》的价格多少？',\n",
       " 'entities': [{'entity': 'ENT', 'start': 1, 'end': 4, 'value': '高等数学'}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ner = BERT_LSTM_NER(bert_ner_config)\n",
    "bert_ner.restore('kbqa/output_ner/')\n",
    "bert_ner.inference(\"《高等数学》的价格多少？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语义匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"加载\"\"\"\n",
    "    data_df = pd.read_csv(file_path, sep='\\t', header=None, names=['question', 'attribute', 'label'])\n",
    "    sent_pairs = data_df.apply(lambda row: (row.question, row.attribute), axis=1).tolist()\n",
    "    labels = data_df.apply(lambda row: int(row.label), axis=1).tolist()\n",
    "\n",
    "    return sent_pairs, np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('请问有没有其他出版社出版了东京暗鸦？', '版权信息'), 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = load_data(\"./kbqa/Data/Sim_Data/train.txt\")\n",
    "x_valid, y_valid = load_data(\"./kbqa/Data/Sim_Data/dev.txt\")\n",
    "x_test, y_test = load_data(\"./kbqa/Data/Sim_Data/test.txt\")\n",
    "\n",
    "x_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Sim_Config():\n",
    "    bert_dir = \"./kbqa/bert/bert_zh_L-12_H-768_A-12_2\" #预训练bert模型的路径\n",
    "    max_seq_length = 128 # 最大序列长度，用于padding成统一的长度\n",
    "    epochs = 5\n",
    "    batch_size = 256\n",
    "    \n",
    "bert_sim_config = BERT_Sim_Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sim_Preprocessor(object):\n",
    "    \"\"\"\n",
    "    预处理类，用于处理转换用户输入，把文字转换成对应的id\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        初始化，加载分词器，此处是按字符切词\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self._tokenizer = bert_tokenization.FullTokenizer(vocab_file=config.bert_dir+'/assets/vocab.txt', do_lower_case=False)\n",
    "        \n",
    "    def tokenize(self, sent1, sent2):\n",
    "        \"\"\"\n",
    "        句对分词\n",
    "        Args:\n",
    "            sent1: 句子1， e.g: \"我爱中国\"\n",
    "            sent2: 句子2， e.g.: \"我爱杭州\"\n",
    "        Return:\n",
    "            ['[CLS]', '我', '爱', '中', '国', '[SEP]', '我', '爱', '杭', '州', '[SEP]']\n",
    "        \"\"\"\n",
    "        stokens_1 = self._tokenizer.tokenize(sent1)\n",
    "        stokens_2 = self._tokenizer.tokenize(sent2)\n",
    "        return ['[CLS]'] + stokens_1 + ['[SEP]'] + stokens_2 + ['[SEP]']\n",
    "        \n",
    "    def get_masks(self, tokens):\n",
    "        \"\"\"获取 BERT mask id 输入\"\"\"\n",
    "\n",
    "        return [1]*len(tokens)\n",
    "\n",
    "    def get_segments(self, tokens):\n",
    "        \"\"\"获取 BERT segments id 输入，第一句用0表示，第二句用1表示\"\"\"\n",
    "\n",
    "        segments = []\n",
    "        current_segment_id = 0\n",
    "        for token in tokens:\n",
    "            segments.append(current_segment_id)\n",
    "            if token == \"[SEP]\":\n",
    "                current_segment_id = 1\n",
    "        return segments\n",
    "\n",
    "    def get_ids(self, tokens):\n",
    "        \"\"\"获取 BERT 字符id 输入\"\"\"\n",
    "        token_ids = [self._tokenizer.vocab.get(token, self._tokenizer.vocab['[UNK]']) for token in tokens]\n",
    "\n",
    "        return token_ids\n",
    "    \n",
    "    def transform(self, sent1, sent2):\n",
    "        \"\"\"将句对转换成BERT需要的字符id、mask id、segments id\"\"\"\n",
    "        \n",
    "        stokens = self.tokenize(sent1, sent2)\n",
    "\n",
    "        input_ids = self.get_ids(stokens)\n",
    "        input_masks = self.get_masks(stokens)\n",
    "        input_segments = self.get_segments(stokens)\n",
    "        \n",
    "        return input_ids, input_masks, input_segments\n",
    "        \n",
    "    def batch_transform(self, sent_pair_list, max_len=None):\n",
    "        \"\"\"批量转换\"\"\"\n",
    "\n",
    "        batch_input_ids, batch_input_masks, batch_input_segments = [], [], []\n",
    "        for sent1, sent2 in sent_pair_list:\n",
    "            input_ids, input_masks, input_segments = self.transform(sent1, sent2)\n",
    "            batch_input_ids.append(input_ids)\n",
    "            batch_input_masks.append(input_masks)\n",
    "            batch_input_segments.append(input_segments)\n",
    "        \n",
    "        batch_input_ids = tf.keras.preprocessing.sequence.pad_sequences(batch_input_ids, maxlen=self.config.max_seq_length, padding='post')\n",
    "        batch_input_masks = tf.keras.preprocessing.sequence.pad_sequences(batch_input_masks, maxlen=self.config.max_seq_length, padding='post')\n",
    "        batch_input_segments = tf.keras.preprocessing.sequence.pad_sequences(batch_input_segments, maxlen=self.config.max_seq_length, padding='post')\n",
    "        \n",
    "            \n",
    "        return [batch_input_ids, batch_input_masks, batch_input_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "按字符切分：\n",
      "['[CLS]', '我', '爱', '中', '国', '[SEP]', '我', '爱', '杭', '州', '[SEP]']\n",
      "把字符编码成对应的字符id、mask id以及句段id：\n",
      "([101, 2769, 4263, 704, 1744, 102, 2769, 4263, 3343, 2336, 102], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "sim_preprocessor = Sim_Preprocessor(bert_sim_config)\n",
    "\n",
    "sent1 = \"我爱中国\"\n",
    "sent2 = \"我爱杭州\"\n",
    "print(\"按字符切分：\")\n",
    "print(sim_preprocessor.tokenize(sent1, sent2))\n",
    "print(\"把字符编码成对应的字符id、mask id以及句段id：\")\n",
    "print(sim_preprocessor.transform(sent1, sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSim(object):\n",
    "    \"\"\"模型类定义\"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        Args:\n",
    "            config: 配置参数\n",
    "        \"\"\"\n",
    "        self._config = config\n",
    "        self._preprocessor = Sim_Preprocessor(config)\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"搭建模型架构\"\"\"\n",
    "        max_seq_length = self._config.max_seq_length\n",
    "        input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                               name=\"input_word_ids\")\n",
    "        input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                           name=\"input_mask\")\n",
    "        segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                            name=\"segment_ids\")\n",
    "        bert_layer = hub.KerasLayer(self._config.bert_dir, trainable=False)\n",
    "        pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "        \n",
    "        dense = tf.keras.layers.Dense(units=128, activation='relu')(pooled_output)\n",
    "        output = tf.keras.layers.Dense(units=1, activation='sigmoid')(dense)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=output)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        self.model = model\n",
    "          \n",
    "    def fit(self, x_train, y_train, x_valid=None, y_valid=None, **kwargs):\n",
    "        \"\"\"\n",
    "        模型训练\n",
    "        Args:\n",
    "            x_data: list[(sent1, sent2)]\n",
    "            y_data: list[label]\n",
    "        \"\"\"\n",
    "        self.build_model() # 搭建模型\n",
    "        self.model.summary()\n",
    "        x_train = self._preprocessor.batch_transform(x_train) #输入预处理\n",
    "        if x_valid is not None and y_valid is not None:\n",
    "            x_valid = self._preprocessor.batch_transform(x_valid)\n",
    "            self.model.fit(\n",
    "                x_train,\n",
    "                y_train,\n",
    "                validation_data=[x_valid, y_valid],\n",
    "                epochs=self._config.epochs,\n",
    "                batch_size=self._config.batch_size,\n",
    "                shuffle=True,\n",
    "                **kwargs) #训练\n",
    "            \n",
    "        else:\n",
    "            self.model.fit(\n",
    "                x_train,\n",
    "                y_train,\n",
    "                epochs=self._config.epochs,\n",
    "                batch_size=self._config.batch_size,\n",
    "                shuffle=True,\n",
    "                **kwargs) #训练\n",
    "            \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        \"\"\"测试集评估\"\"\"\n",
    "        x_test = self._preprocessor.batch_transform(x_test)\n",
    "        self.model.evaluate(x_test, y_test, batch_size=self._config.batch_size)\n",
    " \n",
    "    def predict_similarity(self, sent1, sent2):\n",
    "        \"\"\"\n",
    "        预测两个句子的相似度\n",
    "        \"\"\"\n",
    "        input_ids, input_masks, input_segments = self._preprocessor.transform(sent1, sent2)\n",
    "        \n",
    "        ## 输入数据padding成统一长度\n",
    "        input_ids = tf.keras.preprocessing.sequence.pad_sequences([input_ids], maxlen=self._config.max_seq_length, padding='post')\n",
    "        input_masks = tf.keras.preprocessing.sequence.pad_sequences([input_masks], maxlen=self._config.max_seq_length, padding='post')\n",
    "        input_segments = tf.keras.preprocessing.sequence.pad_sequences([input_segments], maxlen=self._config.max_seq_length, padding='post')\n",
    "        \n",
    "        result = self.model.predict([input_ids, input_masks, input_segments])[0][0]\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def save(self, model_save_dir):\n",
    "        \"\"\"\n",
    "        保存模型\n",
    "        \"\"\"\n",
    "        \n",
    "        model_path = os.path.join(model_save_dir, \"model.h5\")\n",
    "        self.model.save(model_path)\n",
    "    \n",
    "    @classmethod\n",
    "    def restore(self, model_dir):\n",
    "        \"\"\"\n",
    "        加载模型\n",
    "        \"\"\"\n",
    "        self.model=load_model(os.path.join(model_dir, \"model.h5\"), custom_objects={'KerasLayer':hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_word_ids (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " keras_layer_2 (KerasLayer)     [(None, 768),        102267649   ['input_word_ids[0][0]',         \n",
      "                                 (None, 128, 768)]                'input_mask[0][0]',             \n",
      "                                                                  'segment_ids[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          98432       ['keras_layer_2[0][0]']          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 102,366,210\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 102,267,649\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "296/296 [==============================] - 6298s 21s/step - loss: 0.0961 - accuracy: 0.9668 - val_loss: 0.0893 - val_accuracy: 0.9713\n",
      "Epoch 2/5\n",
      "296/296 [==============================] - 5985s 20s/step - loss: 0.0773 - accuracy: 0.9744 - val_loss: 0.0931 - val_accuracy: 0.9691\n",
      "Epoch 3/5\n",
      "296/296 [==============================] - 5965s 20s/step - loss: 0.0737 - accuracy: 0.9756 - val_loss: 0.0819 - val_accuracy: 0.9736\n",
      "Epoch 4/5\n",
      "296/296 [==============================] - 6232s 21s/step - loss: 0.0716 - accuracy: 0.9768 - val_loss: 0.0832 - val_accuracy: 0.9737\n",
      "Epoch 5/5\n",
      "296/296 [==============================] - 6435s 22s/step - loss: 0.0702 - accuracy: 0.9770 - val_loss: 0.0822 - val_accuracy: 0.9741\n"
     ]
    }
   ],
   "source": [
    "bert_sim = BertSim(bert_sim_config)\n",
    "bert_sim.fit(x_train, y_train, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232/232 [==============================] - 4135s 18s/step - loss: 0.0823 - accuracy: 0.9736\n"
     ]
    }
   ],
   "source": [
    "bert_sim.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_sim.save('kbqa/output_sim/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.99578923"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_sim = BertSim(bert_sim_config)\n",
    "bert_sim.restore('kbqa/output_sim/') #只有restore()方法没有load()方法\n",
    "bert_sim.predict_similarity(\"《机械设计基础》这本书的作者是谁？\", \"作者\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 知识库问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KBQA(object):\n",
    "    def __init__(self, kb_path, ner_model, semantic_model):\n",
    "        \"\"\"\n",
    "        初始化知识库，实体识别模型，语义匹配模型\n",
    "        Args:\n",
    "            kb_path: 知识库文件路径\n",
    "            ner_model: 实体识别模型\n",
    "            semantic_model: 语义匹配模型\n",
    "        \"\"\"\n",
    "        data_kb_list = []\n",
    "        data_kb = pd.read_csv(kb_path)\n",
    "\n",
    "        for row in data_kb.index:\n",
    "            question = data_kb.loc[row,\"q_str\"]\n",
    "            entity = data_kb.loc[row,\"t_str\"].split(\"|||\")[0].split(\">\")[1].strip()\n",
    "            attribute = data_kb.loc[row, \"t_str\"].split(\"|||\")[1].strip()\n",
    "            answer = data_kb.loc[row, \"t_str\"].split(\"|||\")[2].strip()\n",
    "            \n",
    "            question = question.replace(\"#\", \"\").replace(\"[UNK]\", \"%\").replace(\"\\n\", \"\")\n",
    "            entity = entity.replace(\"#\", \"\").replace(\"[UNK]\", \"%\").replace(\"\\n\", \"\")\n",
    "            attribute = attribute.replace(\"#\", \"\").replace(\"[UNK]\", \"%\").replace(\"\\n\", \"\")\n",
    "            answer = answer.replace(\"#\", \"\").replace(\"[UNK]\", \"%\").replace(\"\\n\", \"\")\n",
    "\n",
    "            data_kb_list.append([question, entity, attribute, answer])\n",
    "\n",
    "        self._data_kb_list = data_kb_list\n",
    "        self._ner_model = ner_model\n",
    "        self._semantic_model = semantic_model\n",
    "\n",
    "    def query(self, question, method=\"kb\"):\n",
    "        \"\"\"\n",
    "        查询\n",
    "        Args:\n",
    "            question: 查询语句\n",
    "            method：kb:直接匹配知识库三元组, faq: 匹配知识库中自带的问句，如果匹配，返回对应的答案\n",
    "        \"\"\"\n",
    "        if len(question) == 0:\n",
    "            print(\"再见啦！\")\n",
    "            return\n",
    "\n",
    "        print('\\n你的问题是:{}'.format(question))\n",
    "\n",
    "        ner_res = self._ner_model.inference(question)\n",
    "        if not ner_res['entities']:\n",
    "            print(\"未找到实体，请检查问句是否包含实体词，或调整实体识别模型，提升识别性能\")\n",
    "            return\n",
    "        \n",
    "        entity_value = ner_res['entities'][0]['value']\n",
    "        print('识别的实体是：{}'.format(entity_value))\n",
    "\n",
    "        ans_range = []\n",
    "        for j in range(len(self._data_kb_list)):\n",
    "            if self._data_kb_list[j][1] == entity_value:\n",
    "                print(\"结果可能来自：\", self._data_kb_list[j])\n",
    "                ans_range.append(self._data_kb_list[j])\n",
    "                \n",
    "        ans = None\n",
    "        ans_base = None\n",
    "        score = 0\n",
    "\n",
    "        for k in range(len(ans_range)):\n",
    "            if method=='faq':\n",
    "                print(\"句子_{}: {}\".format(k+1, ans_range[k][0]))\n",
    "            else:\n",
    "                print(\"\\n知识三元组%d：\"%(k+1),ans_range[k][1],ans_range[k][2],ans_range[k][3])\n",
    "\n",
    "            #非语义匹配\n",
    "            if ans_range[k][2] in question:\n",
    "                print(\"属性“\",ans_range[k][2],\"”在问题中\")\n",
    "                ans_ = 1\n",
    "\n",
    "            #语义匹配\n",
    "            else:\n",
    "                if method==\"faq\":\n",
    "                    ans_ = self._semantic_model.predict_similarity(question, ans_range[k][0])\n",
    "                    print(\"问句-问句相似度为：\", ans_)\n",
    "                else:                \n",
    "                    #ans_ = self._semantic_model.predict_similarity(question, ans_range[k][1]+ans_range[k][2]+ans_range[k][3])\n",
    "                    ans_ = self._semantic_model.predict_similarity(question, ans_range[k][2])\n",
    "                    print(\"问句--属性匹配度为：\", ans_)\n",
    "\n",
    "            if score < ans_:\n",
    "                score = ans_\n",
    "                ans = ans_range[k][3]\n",
    "                ans_base = ans_range[k]\n",
    "        \n",
    "        threshold = 0.8 if method=='faq' else 0.7\n",
    "\n",
    "        if score < threshold:\n",
    "            print(\"\\n\\033[1;31m答案不确定\\033[0m\")\n",
    "        else:\n",
    "            print(\"\\n\\033[1;31m答案是：{}\\033[0m\".format(ans))\n",
    "            print(\"答案来自三元组：\",ans_base[1],ans_base[2],ans_base[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载实体识别模型\n",
    "bert_ner_config = BERT_NER_Config()\n",
    "bert_ner = BERT_LSTM_NER(bert_ner_config)\n",
    "bert_ner.restore('kbqa/output_ner/')\n",
    "\n",
    "# 加载属性映射模型\n",
    "bert_sim_config = BERT_Sim_Config()\n",
    "bert_sim = BertSim(bert_sim_config)\n",
    "bert_sim.restore('kbqa/output_sim/')\n",
    "\n",
    "# 实例化问答对象\n",
    "kbqa = KBQA(kb_path=\"./kbqa/Data/test.csv\", ner_model=bert_ner, semantic_model=bert_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "你的问题是:《机械设计基础》的价格多少\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[['O', 'B-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "识别的实体是：机械设计基础\n",
      "结果可能来自： ['机械设计基础的isbn码是什么？', '机械设计基础', 'isbn', '9787040192094']\n",
      "结果可能来自： ['机械设计基础的定价是多少？', '机械设计基础', '定价', '24.50元']\n",
      "\n",
      "知识三元组1： 机械设计基础 isbn 9787040192094\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f583c46bee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 884ms/step\n",
      "问句--属性匹配度为： 0.16088037\n",
      "\n",
      "知识三元组2： 机械设计基础 定价 24.50元\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "问句--属性匹配度为： 0.9485226\n",
      "\n",
      "\u001b[1;31m答案是：24.50元\u001b[0m\n",
      "答案来自三元组： 机械设计基础 定价 24.50元\n"
     ]
    }
   ],
   "source": [
    "kbqa.query('《机械设计基础》的价格多少', method='kb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "你的问题是:《机械设计基础》的价格多少\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "[['O', 'B-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'I-ENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "识别的实体是：机械设计基础\n",
      "结果可能来自： ['机械设计基础的isbn码是什么？', '机械设计基础', 'isbn', '9787040192094']\n",
      "结果可能来自： ['机械设计基础的定价是多少？', '机械设计基础', '定价', '24.50元']\n",
      "句子_1: 机械设计基础的isbn码是什么？\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "问句-问句相似度为： 0.9989245\n",
      "句子_2: 机械设计基础的定价是多少？\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "问句-问句相似度为： 0.9994774\n",
      "\n",
      "\u001b[1;31m答案是：24.50元\u001b[0m\n",
      "答案来自三元组： 机械设计基础 定价 24.50元\n"
     ]
    }
   ],
   "source": [
    "kbqa.query('《机械设计基础》的价格多少', method='faq')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
